{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll go through a very basic example of reconstructing, preprocessing, and analyzing 3D face data from video data using Medusa. For more in depth information and examples, check out the other tutorials!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a short video to reconstruct, shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # need 'egl' for rendering!\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
    "from IPython.display import Video\n",
    "from medusa.data import get_example_video\n",
    "\n",
    "vid = get_example_video()\n",
    "Video(vid, embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we'll use the [Mediapipe Face Mesh](https://google.github.io/mediapipe/solutions/face_mesh.html) model to reconstruct the face in the video in 3D. We are going to use the high-level `videorecon` function from Medusa, which reconstructs the video frame by frame and returns a ``MediapipeData`` object, which contains all reconstruction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.preproc import videorecon\n",
    "data = videorecon(vid, recon_model_name='mediapipe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's inspect the ``data`` variable. The reconstructed vertices are stored in the attribute `v`, a 3D numpy array of shape $T$ (time points) $\\times\\ V$ (vertices) $\\times\\ 3$ (X, Y, Z):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"`v` is of type: \", type(data.v))\n",
    "print(\"`v` has shape: \", data.v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The the data contained in `v` represents, for each time point, the 3D coordinates of the vertices (\"points\") that describe the shape of the face. A nice way to visualize these vertices is as a \"wireframe\" on top of the original video. Each data object in Medusa has a ``render_video`` method that renders the reconstructed data as a video. \n",
    "\n",
    "We do this below. By setting the `video` parameter to the path of the video, we tell the `render_video` method to render the wireframe on top of the original video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = './example_vid_recon.mp4'\n",
    "data.render_video(f_out, wireframe=True, video=vid)\n",
    "Video('./example_vid_recon.mp4', embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! However, there are two problems with the data as it is now. First, each vertex represents both \"global\" or \"rigid\" movement (i.e., the face moving left/right/up/down and rotating) and \"local\" or \"non-rigid\" information (i.e., facial expressions such as smiling and frowning). Second, part of these rigid movements seem to reflect noisy \"jitter\", which are simply inaccuracies in the reconstruction.\n",
    "\n",
    "We can separate global and local movement by *aligning* the reconstructions across time, not unlike how motion correction is done in functional MRI preprocessing. We can use the `align` function from Medusa for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.preproc import align\n",
    "data = align(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize it again. By default, Medusa aligns the data to the first time point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = './example_vid_recon.mp4'\n",
    "data.render_video(f_out, wireframe=True, video=vid)\n",
    "Video('./example_vid_recon.mp4', embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further preprocess the data by applying a temporal filter using the `filter` function from Medusa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.preproc import filter\n",
    "data = filter(data, low_pass=4, high_pass=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's render it without the original video in the background (as it's not aligned anymore, anyway) and let's render a smooth mesh instead of a wireframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = './example_vid_recon.mp4'\n",
    "data.render_video(f_out, smooth=False, video=None)\n",
    "Video('./example_vid_recon.mp4', embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot more functionality in Medusa, including different reconstruction models, additional preprocessing functions, and analysis options. A great way to explore this is to check out the tutorials!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5a37db451e33eb29474ac52e4edb908f4b3931f10e8cd1adb5e8d6f1c0c54fc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('gmfx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
