{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering\n",
    "In this tutorial, we'll explain the ins and outs of rendering: the process of creating an image from 3D data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4D-to-video rendering\n",
    "\n",
    "In Medusa, we try to make it as easy as possible to render 4D reconstruction data as a video. As you might have seen in the [quickstart](../getting_started/quickstart), you can use a `VideoRenderer` object for this. Note that this renderer is only available if you have [pytorch3d](https://pytorch3d.org/) installed, which is unfortunately not possible on Windows at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.render import VideoRenderer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class constructor takes three arguments &mdash; `shading`, `lights`, and `loglevel`. We'll ignore the `lights` argument, which we'll discuss later.\n",
    "\n",
    "The `shading` argument can be either \"flat\", which creates a faceted look, or \"smooth\", which creates a smoother surface using [Phong shading](https://en.wikipedia.org/wiki/Phong_shading). We'll use smooth shading for now and set the loglevel to 'WARNING' (which does not output a progress bar which clutters the website):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer = VideoRenderer(shading='smooth', loglevel='WARNING')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The renderer expects the 4D reconstruction data to be wrapped in a `Data4D` object (see [data representation tutorial](./data_representation)). Let's load in the 4D reconstruction (by the 'emoca-coarse' model) from our default video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.data import get_example_data4d\n",
    "data_4d = get_example_data4d(load=True, model='emoca-coarse')\n",
    "\n",
    "# We'll slice the Data4D object this way (resulting in only 50 frames/meshes) so that the \n",
    "# rendering is a bit faster for this tutorial\n",
    "data_4d = data_4d[:50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To render the 4D data to a video, you use the `VideoRenderer` object's `__call__` method (i.e., calling `renderer()`). This method has two mandatory arguments:\n",
    "\n",
    "* `f_out`: path where the video will be saved\n",
    "* `data`: the `Data4D` object\n",
    "\n",
    "Additionally, this method accepts two optional arguments:\n",
    "\n",
    "* `video`: path to original video\n",
    "* `overlay`: colors to project onto the vertices before rendering\n",
    "\n",
    "If you set the `video` argument to the original video associated with the 4D reconstruction, then the video will be used as a background on which the 4D data is rendered. The default value is `None` (in case the data will be rendered on a white background). \n",
    "\n",
    "Let's render our data on top of the video!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.data import get_example_video\n",
    "from IPython.display import Video  # just to show the video in the notebook/website\n",
    "\n",
    "vid = get_example_video()\n",
    "f_out = './smooth.mp4'\n",
    "\n",
    "renderer(f_out, data_4d, video=vid)\n",
    "\n",
    "# Show result\n",
    "Video(f_out, embed=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to make the visualization a little nicer is by only rendering the face (rather than the full head). To do so, you can use the `apply_vertex_mask` method from the `Data4D` object with the `name` argument set to `'face'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4d.apply_vertex_mask('face')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method basically removes all non-face vertices from the mesh, leaving us with 1787 vertices (instead of the original 5023):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(data_4d.v.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's re-render the data (which is a lot faster now too, as it has to work with fewer vertices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer(f_out, data_4d, video=vid)\n",
    "Video(f_out, embed=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlays\n",
    "\n",
    "So far, we only rendered the face as a grayish, untextured shape. We can, however, give it a different uniform color or specific color per vertex. To do so, we can give the `__call__` function an additional argument, `overlay`, a tensor with a single color per vertex. Colors need to be represented as RGB float values ranging from 0 to 1, so overlay should be a $V \\times 3$ tensor. \n",
    "\n",
    "To demonstrate, let's create an overlay that'll make the face bright red, which corresponds to an RGB value of [1., 0., 0.]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "V = data_4d.v.shape[1]\n",
    "vertex_colors = torch.zeros((V, 3), device=data_4d.device)\n",
    "vertex_colors[:, 0] = 1.  # set 'red' channel to 1\n",
    "\n",
    "tuple(vertex_colors.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's render the video again, but now with `vertex_colors` used for the `overlay` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = './test.mp4'\n",
    "renderer(f_out, data_4d, overlay=vertex_colors)\n",
    "\n",
    "Video(f_out, embed=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we don't have to use the same colors for each time point! We can also create an overlay of shape $N$ (nr of frames) $\\times V \\times 3$, which specifies a specific color for each frame (and each vertex). To demonstrate, we'll generate random RGB values for each frame and vertex, creating quite a trippy visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = data_4d.v.shape[0]\n",
    "vertex_colors = torch.rand(N, V, 3, device='cuda')\n",
    "renderer(f_out, data_4d, overlay=vertex_colors)\n",
    "\n",
    "Video(f_out, embed=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that we would like to color the face along a more interesting feature, like the amount of local movement relative to the first frame (which we assume represents a neutral face). This is in fact quite an extensive procedure. We'll walk you through this procedure step by step, but aftwards we'll show you a way to do this more easily.\n",
    "\n",
    "First, each frame's movement relative to the first frame ($\\delta v_{i}$) can be computed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta v_{i} = v_{i} - v_{0}\n",
    "\\end{equation}\n",
    "\n",
    "Importantly, we assume for now that we're only interesting in the local movement of the face (i.e., facial expressions) rather than the global movement (i.e., rotation and movement of the entire head). To project out the global movement, we can call the `to_local` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To only show local deviations, we can use the to_local() method which projects out any \"global\" movement\n",
    "data_4d.to_local()\n",
    "dv = data_4d.v - data_4d.v[0, :, :]\n",
    "tuple(dv.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is that we do not have one value for each vertex to visualize, but three: the movement in the X (left-right), Y (up-down), and Z (forward-backward) direction! We could of course just visualize a single direction, but another possibility is that we project the movement on the [vertex normals](https://en.wikipedia.org/wiki/Vertex_normal): the direction perpendicular to the mesh at each vertex (see image below).\n",
    "\n",
    "![vertex_normals](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Vertex_normals.png/220px-Vertex_normals.png)\n",
    "\n",
    "*Red lines represent the vertex normals; from [wikipedia](https://en.wikipedia.org/wiki/Vertex_normal)*\n",
    "\n",
    "We can get the vertex normals and project the movement (`dv`) onto them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.geometry import compute_vertex_normals\n",
    "\n",
    "# normals: V (1787) x 3 (XYZ)\n",
    "normals = compute_vertex_normals(data_4d.v[0], data_4d.tris)\n",
    "dv_proj = (normals * dv).sum(dim=2)\n",
    "\n",
    "tuple(dv_proj.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projected data (`dv_proj`) now represents movement relative to the normal direction: positive values indicate that movement occurs in the same direction as the normal (i.e., \"outwards\") and negative values indicate the movement occurs in the direction opposite to the normal (i.e., \"inwards\").\n",
    "\n",
    "Now, the only step remaining is to convert the values (`dv_proj`) to RGB colors. Let's say we want to show inward movement as blue and outward movement as red; we can use `matplotlib` for this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import CenteredNorm\n",
    "from matplotlib import colormaps\n",
    "\n",
    "norm = CenteredNorm(vcenter=0.)  # will make sure that 0 is in the \"middle\" of the colormap\n",
    "cmap = colormaps['bwr']  # blue-white-red colormap\n",
    "\n",
    "# the colormap does not accept torch tensors\n",
    "dv_proj = dv_proj.cpu().numpy()\n",
    "dv_proj_colors = cmap(norm(dv_proj))\n",
    "\n",
    "# convert back to torch tensor\n",
    "dv_proj_colors = torch.as_tensor(dv_proj_colors, device=data_4d.device).float()\n",
    "\n",
    "# N x V x RGBA (but the 'A' will be discarded later)\n",
    "tuple(dv_proj_colors.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can pass these colors (`dv_proj_colors`) to the renderer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer = VideoRenderer(shading='smooth', loglevel='WARNING')\n",
    "renderer(f_out, data_4d, overlay=dv_proj_colors)\n",
    "\n",
    "Video(f_out, embed=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll probably agree that this entire process is quite cumbersome. To make things a little easier, Medusa provides an `Overlay` class that performs much of the boilerplate code necessary to render such overlays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.render import Overlay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important arguments when initializing an `Overview` object are:\n",
    "\n",
    "* `v`: the vertex values that will be used to create colors (e.g., the `dv` variable from earlier)\n",
    "* `cmap`: a string with the [Matplotlib colormap](https://matplotlib.org/stable/tutorials/colors/colormaps.html) that will be used (default: `'bwr'`)\n",
    "* `dim`: dimension of `v` that will be plotted (0 for X, 1 for Y, 2 for Z, or 'normals')\n",
    "\n",
    "If you want to project the XYZ values onto the vertex normals (by setting `dim='normals'`), then you also need to provide the vertices of a neutral frame (`v0`) and the mesh triangles (`tris`), as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = Overlay(dv, cmap='bwr', vcenter=0., dim='normals', v0=data_4d.v[0], tris=data_4d.tris)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the colors, simply call the `to_rgb` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = overlay.to_rgb()\n",
    "tuple(colors.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally render as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4d.to_world()\n",
    "renderer(f_out, data_4d, overlay=colors)\n",
    "\n",
    "Video(f_out, embed=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D-to-image rendering\n",
    "\n",
    "A lot of the steps necessary to render each 3D mesh from the 4D sequence is abstracted away in the `VideoRender` class. If you want to know more or want more flexibility with respect to rendering, keep on reading!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, what the `VideoRender` class does is looping over all frames in the video, fetching the 3D mesh(es) per frame (remember, there may be more than one face, and thus more than one mesh, per frame!), render them to an image, which are written to disk as a continuous video file.\n",
    "\n",
    "Rendering happens using the `PytorchRenderer` class, a thin wrapper around functionality from the `pytorch3d` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.render import PytorchRenderer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this renderer only renders (batches of) 3D meshes to _images_. It is initialized with a three mandatory arguments:\n",
    "* `viewport` (a tuple with two integers): the output size of the rendered image (width x height)\n",
    "* `cam_mat` (a 4x4 tensor or numpy array): the affine matrix that defines the camera pose\n",
    "* `cam_type` (a string): the type of camera\n",
    "\n",
    "The `cam_type` is \"orthographic\" for any FLAME-based reconstruction data and \"perspective\" for Mediapipe data. The `cam_mat` and `viewport` data can be extracted from the `Data4D` object you intend to render:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewport = data_4d.video_metadata['img_size']\n",
    "renderer = PytorchRenderer(viewport, cam_mat=data_4d.cam_mat, cam_type='orthographic')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can pass any single or batch of 3D meshes to the renderer's `__call__` function (together with the mesh's triangles) which will return an image with the mesh(es) rendered onto it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render the first time point\n",
    "img = renderer(data_4d.v[0], data_4d.tris)\n",
    "\n",
    "tuple(img.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the returned image (`img`) is of shape $N$ (number of images) $\\times H$ (height) $\\times W$ (width) $\\times 4$ (RGBA) with unsigned integers (0-255). Note that you can also render _multiple_ meshes to _multiple_ images at the same time by explicitly setting the `single_image` argument to `False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render the first 16 time points\n",
    "imgs = renderer(data_4d.v[:16], data_4d.tris, single_image=False)\n",
    "tuple(imgs.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to render the 16 rendered meshes on top of the original video frames, we can load in the video frames and \"blend\" it with the video frames. To load the video frames in memory, we can use the `VideoLoader` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.data import get_example_video\n",
    "from medusa.io import VideoLoader\n",
    "\n",
    "vid = get_example_video()\n",
    "loader = VideoLoader(vid, batch_size=16)\n",
    "\n",
    "# To only get a single batch, you can create an iterator manually and call next() on it\n",
    "# (bg = background)\n",
    "bg = next(iter(loader))\n",
    "\n",
    "tuple(bg.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to blend the rendered meshes with the video, we can call the `alpha_blend` method from the renderer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_with_bg = renderer.alpha_blend(imgs, bg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write the rendered images (which are torch tensors) to a video file, we can use the `VideoWriter` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.io import VideoWriter\n",
    "\n",
    "writer = VideoWriter('./test.mp4', fps=loader.get_metadata()['fps'])\n",
    "writer.write(img_with_bg)\n",
    "writer.close()  # call close if you're done!\n",
    "\n",
    "Video('./test.mp4', embed=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we're getting the same result as with the `VideoRenderer` approach (albeit with only 16 frames)! Although it requires more boilerplate code, this approach gives you more flexibility to render the data exactly as you want it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medusa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
