{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering\n",
    "In this tutorial, we'll explain the ins and outs of rendering: the process of creating an image from 3D data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4D-to-video\n",
    "\n",
    "In Medusa, we try to make it as easy as possible to visualize 4D reconstruction data. As you might have seen in the [quickstart](../getting_started/quickstart), you can use a `VideoRenderer` object for this. Note that this renderer is only available if you have [pytorch3d](https://pytorch3d.org/) installed, which is unfortunately not possible on Windows at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.render import VideoRenderer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class constructor takes three arguments &mdash; `render_cls`, `shading`, and `lights`. We can ignore the `render_cls` argument (as there is only one render class supported, i.e., `PytorchRenderer`) and the `lights` argument, which we'll discuss later. \n",
    "\n",
    "The `shading` argument can be either \"flat\", which creates a faceted look, or \"smooth\", which creates a smoother surface using [Phong shading](https://en.wikipedia.org/wiki/Phong_shading). We'll use flat shading for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer = VideoRenderer(shading='flat')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The renderer expects the 4D reconstruction data to be wrapped in a `Data4D` object (see [data representation tutorial](./data_representation)). Let's load in the 4D reconstruction (by the 'emoca-coarse' model) from our default video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.data import get_example_h5\n",
    "data_4d = get_example_h5(load=True, model='emoca-coarse')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To render the 4D data to a video, you use the `VideoRenderer` object's `__call__` method (i.e., calling `renderer()`). This method has two mandatory arguments:\n",
    "\n",
    "* `f_out`: path where the video will be saved\n",
    "* `data`: the `Data4D` object\n",
    "\n",
    "Additionally, this method accepts two optional arguments:\n",
    "\n",
    "* `video`: path to original video\n",
    "* `overlay`: colors to project onto the vertices before rendering\n",
    "\n",
    "If you set the `video` argument to the original video associated with the 4D reconstruction, then the video will be used as a background on which the 4D data is rendered. The default value is `None` (in case the data will be rendered on a white background). \n",
    "\n",
    "Let's render our data on top of the video!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.data import get_example_video\n",
    "from IPython.display import Video\n",
    "\n",
    "vid = get_example_video()\n",
    "f_out = './flat.mp4'\n",
    "\n",
    "renderer(f_out, data_4d, video=vid)\n",
    "\n",
    "# Show result\n",
    "#Video(f_out, embed=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to make the visualization a little nicer is by only rendering the face (rather than the full head). To do so, you can use the `apply_vertex_mask` method from the `Data4D` object with the `name` argument set to `'face'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4d.apply_vertex_mask('face')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method basically removes all non-face vertices from the mesh, leaving us with 1787 vertices (instead of the original 5023):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(data_4d.v.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's re-render the data (which is a lot faster now too, as it has to work with fewer vertices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer(f_out, data_4d, video=vid)\n",
    "Video(f_out, embed=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.render import Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4d.to_local()\n",
    "dev = data_4d.v - data_4d.v[0, :, :]\n",
    "overlay = Overlay(dev, colormap='bwr')\n",
    "colors = overlay.to_array(data_4d.v[0], tris=data_4d.tris)\n",
    "renderer = VideoRenderer(shading='flat')\n",
    "renderer(f_out, data_4d, overlay=colors)\n",
    "Video(f_out, embed=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D-to-image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medusa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
