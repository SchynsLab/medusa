{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll go through a very basic example of reconstructing, preprocessing, and visualizing 4D faces from video data using Medusa's Python API. For more information about its command-line interface, check the CLI [documentation](../api/cli)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a short video to reconstruct, shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "from medusa.data import get_example_video\n",
    "\n",
    "# Returns path to an example mp4 file\n",
    "vid = get_example_video()\n",
    "print(f\"Going to reconstruct {vid.name}!\")\n",
    "\n",
    "# Show in notebook\n",
    "Video(vid, embed=True)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction\n",
    "\n",
    "For this example, we'll use the [Mediapipe Face Mesh](https://google.github.io/mediapipe/solutions/face_mesh.html) model to reconstruct the face in the video in 4D, that is, a 3D reconstruction for each frame of the video. We are going to use the high-level `videorecon` function from Medusa, which reconstructs the video frame by frame and returns a ``Data4D`` object, which contains all reconstruction (meta)data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-stdout"
    ]
   },
   "outputs": [],
   "source": [
    "from medusa.recon import videorecon\n",
    "data = videorecon(vid, recon_model='mediapipe', loglevel='WARNING')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's inspect the ``data`` variable. The reconstructed vertices are stored in the attribute `v`, a [PyTorch](https://pytorch.org/) tensor of shape $T$ (time points) $\\times\\ V$ (vertices) $\\times\\ 3$ (X, Y, Z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"`v` is of type: \", type(data.v))\n",
    "print(\"`v` has shape: \", tuple(data.v.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The the data contained in `v` represents, for each time point, the 3D coordinates of the vertices (also called \"landmarks\") that describe the shape of the face. The particular mesh used by Mediapipe contains 468 vertices, but other reconstruction models may contain many more vertices (like [FLAME](https://flame.is.tue.mpg.de)-based models, which reconstruct 5023 vertices)!\n",
    "\n",
    "To get an idea of the data, let's just extract the 3D vertices from the first time point (i.e., the first frame of the video) and plot it. We need to do this in 2D, of course, so we'll just a scatterplot to visualize the X and Y coordinates only:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "v = data.v.cpu().numpy()  # move data from torch to numpy!\n",
    "t0 = v[0, :, :]  # first time point\n",
    "t0_x = t0[:, 0]\n",
    "t0_y = t0[:, 1]\n",
    "\n",
    "plt.figure(figsize=(6, 6), )\n",
    "plt.scatter(t0_x, t0_y)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more appealing way to visualize the reconstruction is as a \"wireframe\". Medusa allows you to do this for all time points, such that it creates a video of the full 4D reconstruction, and (optionally) rendered on top of the original video as well. To do so, you can use the ``render_video`` method that each data object in Medusa has. \n",
    "\n",
    "We do this below. By setting the `video` parameter to the path of the video, we tell the `render_video` method to render the wireframe on top of the original video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.render import VideoRenderer\n",
    "\n",
    "renderer = VideoRenderer(shading='flat')\n",
    "f_out = './example_vid_recon.mp4'\n",
    "renderer(f_out, data, video=vid)\n",
    "\n",
    "# Show in notebook\n",
    "Video('./example_vid_recon.mp4', embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! However, there are two issues with the data as it is now. First, each vertex represents both \"global\" (rigid) movement (i.e., the face moving left/right/up/down and rotating) and \"local\" (non-rigid) information (i.e., facial expressions such as smiling and frowning). Second, part of these rigid movements seem to reflect noisy \"jitter\", which are simply inaccuracies in the reconstruction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment\n",
    "\n",
    "We can separate global and local movement by *aligning* the reconstructions across time. Alignment, here, refers to the rotation and translation necessary to match the reconstructed vertices from each timepoint to a reference timepoint or template. The alignment parameters were already estimated during reconstruction and are stored in the `.mat` attribute of the data object, a tensor of shape $T \\times 4 \\times 4$, representing a $4 \\times 4$ affine matrix for each of the $T$ timepoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(data.mat.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before alignment, the data is in so-called \"world space\". After alignment to a template mesh (centered at the origin of its local coordinate system), the data is in \"local space\". To move the data between these two spaces, you can use the `to_local` method (which projects out global motion) and `to_world` methods (which adds global motion back in). \n",
    "\n",
    "Let's get rid of the global motion and see what the reconstruction now looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_local()\n",
    "\n",
    "f_out = './example_vid_recon.mp4'\n",
    "renderer = VideoRenderer(shading='flat')\n",
    "renderer(f_out, data)\n",
    "\n",
    "Video('./example_vid_recon.mp4', embed=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, we can convert this matrix representation of global motion to a set of translation and rotation parameters (and shear and scale parameters, which we ignore for now) that are easier to interpret. To do this, you can use the `decompose_mats` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose affine matrices to movement parameters\n",
    "motion_params = data.decompose_mats()\n",
    "\n",
    "# Select translation and rotation only (ignore shear/scale)\n",
    "motion_params = motion_params.iloc[:, :6]\n",
    "\n",
    "# Show first five time points (i.e., video frames)\n",
    "motion_params.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the vertices, these parameters can be interpreted as time series representing the rigid movement of the face over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show movement relative to first timepoint\n",
    "motion_params = motion_params - motion_params.iloc[0, :]\n",
    "trans_params = motion_params.iloc[:, :3]\n",
    "rot_params = motion_params.iloc[:, 3:]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, sharex=True, figsize=(12, 6))\n",
    "axes[0].plot(trans_params)\n",
    "axes[0].set_ylabel('Translation (in mm.)', fontsize=15)\n",
    "axes[0].set_xlim(0, motion_params.shape[0])\n",
    "axes[1].plot(rot_params)\n",
    "axes[1].set_ylabel('Rotation (in deg.)', fontsize=15)\n",
    "axes[1].set_xlabel('Frame nr.', fontsize=15)\n",
    "axes[1].legend(['X', 'Y', 'Z'], frameon=False, ncol=3, fontsize=15)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot more functionality in Medusa, including different reconstruction models, preprocessing functions, and analysis options. A great way to explore this is to check out the tutorials. A good starting point is the [4D reconstruction](../tutorials/reconstruction) tutorial. Or browse through the [Python API reference](../api/python)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('medusa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a1fe4ecb158ede289866924cdf5c951eb1c9bf4cc83e0fc26b1138d86cd1726"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
