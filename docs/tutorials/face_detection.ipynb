{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detection\n",
    "\n",
    "Although Medusa's main focus is 3D/4D reconstruction, it also contains functionality for face detection, facial landmark prediction, and cropping as these steps often need to be performed before feeding images into reconstruction models. \n",
    "\n",
    "In this tutorial, we will demonstrate Medusa's face detection functionality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection on (single) images\n",
    "\n",
    "The first step in many face analysis pipelines is _face detection_. Medusa contains two classes that perform face detection:\n",
    "\n",
    "* `SCRFDetector`: a detection model based on InsightFace's SCRFD model `citep`{};\n",
    "* `YunetDetector`: a detection model implemented in OpenCV\n",
    "\n",
    "We recommend using the `SCRFDetector` as our experience is that it is substantially more accurate than `YunetDetector` (albeit a bit slower when run on CPU); if you want to use the `YunetDetector`, make sure to install OpenCV first (`pip install python-opencv`). So we'll use the `SCRFDetector` for the rest of this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.detect import SCRFDetector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, `SCRFDetector` uses an ONNX model provided by InsightFace, but our implementation is quite a bit faster than the original InsightFace implementation as ours uses PyTorch throughout (rather than a mix of PyTorch and numpy).\n",
    "\n",
    "The `SCRFDetector` takes the following inputs upon initialization:\n",
    "\n",
    "* `det_size`: size to resize images to before passing to the detection model;\n",
    "* `det_threshold`: minimum detection threshold (float between 0-1)\n",
    "* `nms_threshold`: non-maximum suppression threshold (boxes overlapping more than this proportion are removed)\n",
    "* `device`: either \"cpu\" or \"cuda\" (determined automatically by default)\n",
    "\n",
    "The most important arguments are `det_size` and `det_threshold`; a higher `det_size` (a tuple with two integers, width x height) leads to potentially more accurate detections but slower processing; increasing `det_threshold` leads to more conservative detections (fewer false alarms, but more misses) and vice versa.\n",
    "\n",
    "In our experience, the defaults are fine for most images/videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = SCRFDetector()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply it to some example data. We'll use a single frame from out example video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.data import get_example_image\n",
    "img = get_example_image(load=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `img` represents is loaded as a PyTorch tensor, but the detectors in Medusa can deal with paths to images or numpy arrays, too. Now, to process this image with the detector, we'll call the `detector` object as if it is a function (which internally triggers the `__call__` method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det = detector(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the detector call, `det`, contains a dictionary with information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, all values of the dictionary are PyTorch tensors. The most important keys are:\n",
    "\n",
    "* `conf`: the confidence of each detection (0-1)\n",
    "* `lms`: a set of five landmark coordinates per detection\n",
    "* `bbox`: a bounding box per detection\n",
    "\n",
    "Let's take a look at `conf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = det['conf']\n",
    "print(f\"Conf: {conf.item():.3f}, shape: {tuple(conf.shape)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for this image, there is only one detection with a confidence of 0.884. Note that there may be more than one detection per image when there are more faces in the image! \n",
    "\n",
    "Now, let's also take a look at the bounding box for the detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det['bbox']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bounding box contains four values (in pixel units) that represent the box' mimimum x-value, minimum y-value, maximum x-value, and maximum y-value (in that order). We can in fact visualize this bounding box quite straightforwardly using `torchvision`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import Image\n",
    "from torchvision.utils import draw_bounding_boxes, save_image\n",
    "\n",
    "# Note that `draw_bounding_boxes` expects the img to be in C x H x W format and uint8, \n",
    "# so squeeze out batch dimension and convert float32 to uint8\n",
    "red = (255, 0, 0)\n",
    "img = img.squeeze(0).to(torch.uint8)\n",
    "img_draw = draw_bounding_boxes(img, det['bbox'], colors=red, width=2)\n",
    "\n",
    "# Save image to disk and display in notebook\n",
    "save_image(img_draw.float(), './viz/bbox.png', normalize=True)\n",
    "Image('./viz/bbox.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a proper bounding box! Now, let's finally look at the predicted facial landmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det['lms']  # B x 5 x 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each detection also comes with 5 landmarks consisting of two values (one for X, one for Y) in pixel units. As we'll show below (again, using `torchvision`), these landmarks refer to the left eye, right eye, tip of the nose, left mouthcorner, and right mouth corner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_keypoints\n",
    "\n",
    "# Note that `draw_keypoints` also expects the img to be in C x H x W format\n",
    "img_draw = draw_keypoints(img, det['lms'], colors=red, radius=4)\n",
    "\n",
    "# Save image to disk and display in notebook\n",
    "save_image(img_draw.float(), './viz/lms.png', normalize=True)\n",
    "Image('./viz/lms.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection on batches of images\n",
    "\n",
    "Thus far, we only applied face detection to a single image, but Medusa's face detectors also work on batches of images such that it can be easily used to process video data, which gives us a good excuse to showcase Medusa's powerful `BatchResults` class (explained later).\n",
    "\n",
    "Let's try this out on our example video, which we load in batches using Medusa's `VideoLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.data import get_example_video\n",
    "from medusa.io import VideoLoader\n",
    "\n",
    "vid = get_example_video()\n",
    "loader = VideoLoader(vid, batch_size=64)\n",
    "\n",
    "# The loader can be used as an iterator (e.g. in a for loop), but here we only\n",
    "# load in a single batch; note that we always need to move the data to the desired\n",
    "# device (CPU or GPU)\n",
    "batch = next(iter(loader))\n",
    "batch = batch.to(loader.device)\n",
    "\n",
    "# B (batch size) x C (channels) x H (height) x W (width)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the detector as usual and call it on the batch of images like we did on a single image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = SCRFDetector()\n",
    "out = detector(batch)\n",
    "\n",
    "print(out.keys())\n",
    "print(out['bbox'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the detection results of this batch of images, we could write a for-loop and use `torchvision` to create for each face/detection and image with the bounding box and face landmarks, but Medusa has a specialized class for this type of batch data to make aggregation and visualization easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.containers import BatchResults\n",
    "from IPython.display import Video\n",
    "\n",
    "# `BatchResults` takes any output from a detector model (or crop model) ...\n",
    "results = BatchResults(**out)\n",
    "\n",
    "# ... which it'll then visualize as a video (or, if video=False, a set of images)\n",
    "results.visualize('./viz/test.mp4', batch, video=True, fps=loader._metadata['fps'])\n",
    "\n",
    "# Embed in notebook\n",
    "Video('./viz/test.mp4', embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BatchResults` class is especially useful when dealing with multiple batches of images (which will be the case for most videos). When dealing with multiple batches, initialize an \"empty\" `BatchResults` object before any processing, and then in each iteration call its `add` method with the results from the detector.\n",
    "\n",
    "Here, we show an example for three consecutive batches; note that `BatchResults` will store anything you give it, so here we're also giving it the raw images (using the `images=batch`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = VideoLoader(get_example_video(n_faces=2))\n",
    "results = BatchResults()\n",
    "for i, batch in enumerate(loader):\n",
    "    batch = batch.to(loader.device)\n",
    "    out = detector(batch)\n",
    "    results.add(images=batch, **out)\n",
    "\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, the `results` object contains for each detection attribute (like `lms`, `conf`, `bbox`, etc) a list with one value for each batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of length 3 (batches), with each 64 values (batch size)\n",
    "results.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can concatenate everything by calling the object's `concat` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.concat()\n",
    "results.conf.shape  # 192 (= 3 * 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can visualize the results as before (note that we give it the raw images as well):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.visualize('./viz/test.mp4', results.images, video=True, fps=loader._metadata['fps'])\n",
    "Video('./viz/test.mp4', embed=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medusa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
